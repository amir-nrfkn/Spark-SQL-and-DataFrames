{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d79c5649",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6630c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a3f3c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc08a2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = config['PATH']['path']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7589c5",
   "metadata": {},
   "source": [
    "- Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe13d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/11 23:07:54 WARN Utils: Your hostname, BigMac.local resolves to a loopback address: 127.0.0.1; using 192.168.0.186 instead (on interface en0)\n",
      "23/01/11 23:07:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/11 23:07:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/11 23:07:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Spark SQL Query Dataframes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f2dcc",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b9e111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_df_path = path + '/utilization.json'\n",
    "df = spark.read.format(\"json\").load(json_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964b3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "|           0.32|03/05/2019 08:56:14|       0.37|      100|           47|\n",
      "|           0.62|03/05/2019 09:01:14|       0.59|      100|           60|\n",
      "|           0.66|03/05/2019 09:06:14|       0.72|      100|           57|\n",
      "|           0.54|03/05/2019 09:11:14|       0.54|      100|           44|\n",
      "|           0.29|03/05/2019 09:16:14|        0.4|      100|           47|\n",
      "|           0.43|03/05/2019 09:21:14|       0.68|      100|           66|\n",
      "|           0.49|03/05/2019 09:26:14|       0.66|      100|           65|\n",
      "|           0.64|03/05/2019 09:31:14|       0.55|      100|           66|\n",
      "|           0.42|03/05/2019 09:36:14|        0.6|      100|           42|\n",
      "|           0.55|03/05/2019 09:41:14|       0.59|      100|           63|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701e7343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7cebb",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a74727",
   "metadata": {},
   "source": [
    "- create temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3fd575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606cd6f",
   "metadata": {},
   "source": [
    "- query the utilization table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03261cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                   SELECT * \n",
    "                   FROM utilization \n",
    "                   LIMIT 10\n",
    "                   \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b4f414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.57|03/05/2019 08:06:14|       0.51|      100|           47|\n",
      "|           0.47|03/05/2019 08:11:14|       0.62|      100|           43|\n",
      "|           0.56|03/05/2019 08:16:14|       0.57|      100|           62|\n",
      "|           0.57|03/05/2019 08:21:14|       0.56|      100|           50|\n",
      "|           0.35|03/05/2019 08:26:14|       0.46|      100|           43|\n",
      "|           0.41|03/05/2019 08:31:14|       0.58|      100|           48|\n",
      "|           0.57|03/05/2019 08:36:14|       0.35|      100|           58|\n",
      "|           0.41|03/05/2019 08:41:14|        0.4|      100|           58|\n",
      "|           0.53|03/05/2019 08:46:14|       0.35|      100|           62|\n",
      "|           0.51|03/05/2019 08:51:14|        0.6|      100|           45|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae56b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                   SELECT cpu_utilization, session_count\n",
    "                   FROM utilization \n",
    "                   LIMIT 10\n",
    "                   \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b26aba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|cpu_utilization|session_count|\n",
      "+---------------+-------------+\n",
      "|           0.57|           47|\n",
      "|           0.47|           43|\n",
      "|           0.56|           62|\n",
      "|           0.57|           50|\n",
      "|           0.35|           43|\n",
      "|           0.41|           48|\n",
      "|           0.57|           58|\n",
      "|           0.41|           58|\n",
      "|           0.53|           62|\n",
      "|           0.51|           45|\n",
      "+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b3583c",
   "metadata": {},
   "source": [
    "### Filtering Dataframes with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b0c1a",
   "metadata": {},
   "source": [
    "- get all records that the CPU utilization went over 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ce2c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT cpu_utilization, free_memory\n",
    "                    FROM utilization\n",
    "                    WHERE cpu_utilization >= 0.5\n",
    "                    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73ba5637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "386739"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "046e568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|cpu_utilization|free_memory|\n",
      "+---------------+-----------+\n",
      "|           0.57|       0.51|\n",
      "|           0.56|       0.57|\n",
      "|           0.57|       0.56|\n",
      "|           0.57|       0.35|\n",
      "|           0.53|       0.35|\n",
      "|           0.51|        0.6|\n",
      "|           0.62|       0.59|\n",
      "|           0.66|       0.72|\n",
      "|           0.54|       0.54|\n",
      "|           0.64|       0.55|\n",
      "|           0.55|       0.59|\n",
      "|           0.61|       0.34|\n",
      "|           0.62|       0.49|\n",
      "|           0.58|       0.54|\n",
      "|           0.63|       0.63|\n",
      "|           0.62|       0.69|\n",
      "|           0.56|       0.69|\n",
      "|           0.62|       0.36|\n",
      "|           0.57|       0.46|\n",
      "|           0.55|       0.42|\n",
      "+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "994b412e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|cpu_utilization|free_memory|\n",
      "+---------------+-----------+\n",
      "|            0.5|        0.5|\n",
      "|           0.67|        0.5|\n",
      "|           0.66|        0.5|\n",
      "|           0.66|        0.5|\n",
      "|           0.54|        0.5|\n",
      "|           0.69|        0.5|\n",
      "|           0.66|        0.5|\n",
      "|           0.56|        0.5|\n",
      "|           0.55|        0.5|\n",
      "|            0.5|        0.5|\n",
      "|           0.57|        0.5|\n",
      "|           0.55|        0.5|\n",
      "|           0.53|        0.5|\n",
      "|            0.6|        0.5|\n",
      "|            0.7|        0.5|\n",
      "|           0.64|        0.5|\n",
      "|           0.67|        0.5|\n",
      "|           0.51|        0.5|\n",
      "|           0.54|        0.5|\n",
      "|           0.51|        0.5|\n",
      "+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT cpu_utilization, free_memory\n",
    "                    FROM utilization\n",
    "                    WHERE cpu_utilization >= 0.5\n",
    "                    AND free_memory <= 0.5\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.sort(df_sql['free_memory'].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a56e498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324839"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbb9d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-------------+\n",
      "|cpu_utilization|free_memory|session_count|\n",
      "+---------------+-----------+-------------+\n",
      "|           0.73|       0.27|          105|\n",
      "|            0.9|       0.01|          105|\n",
      "|           0.66|        0.1|          105|\n",
      "|           0.68|       0.03|          105|\n",
      "|           0.96|       0.24|          105|\n",
      "|           0.92|       0.19|          105|\n",
      "|           0.99|        0.2|          105|\n",
      "|           0.68|        0.1|          105|\n",
      "|           0.98|       0.17|          105|\n",
      "|           0.64|        0.2|          105|\n",
      "|           0.68|       0.26|          105|\n",
      "|           0.88|       0.32|          105|\n",
      "|           0.99|       0.33|          105|\n",
      "|           0.63|       0.24|          105|\n",
      "|            0.6|       0.35|          105|\n",
      "|           0.89|       0.34|          105|\n",
      "|           0.98|       0.23|          105|\n",
      "|           0.86|       0.16|          105|\n",
      "|           0.74|       0.33|          105|\n",
      "|           0.92|       0.33|          105|\n",
      "+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        cpu_utilization, \n",
    "                        free_memory,\n",
    "                        session_count\n",
    "                    FROM \n",
    "                        utilization\n",
    "                    WHERE \n",
    "                        cpu_utilization >= 0.5\n",
    "                    AND \n",
    "                        free_memory <= 0.5\n",
    "                    ORDER BY \n",
    "                        session_count DESC\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65e82b",
   "metadata": {},
   "source": [
    "### Aggregating data using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0347184",
   "metadata": {},
   "source": [
    "- count all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "691fe9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  500000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT count(*)\n",
    "                    FROM utilization\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f696b2",
   "metadata": {},
   "source": [
    "- count how many times CPU utilization went over 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33664ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  386739|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT count(*)\n",
    "                    FROM utilization\n",
    "                    WHERE cpu_utilization >= 0.5\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166679f",
   "metadata": {},
   "source": [
    "- count number of times over 50% CPU utilization and group by server_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "210765df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      103|   10000|\n",
      "|      100|    4280|\n",
      "|      101|   10000|\n",
      "|      102|   10000|\n",
      "|      107|    8896|\n",
      "|      104|   10000|\n",
      "|      106|    3163|\n",
      "|      105|    4950|\n",
      "|      110|    6483|\n",
      "|      108|   10000|\n",
      "|      109|    6704|\n",
      "|      112|   10000|\n",
      "|      113|   10000|\n",
      "|      114|    5859|\n",
      "|      111|    6600|\n",
      "|      116|    5099|\n",
      "|      115|    8630|\n",
      "|      117|    7212|\n",
      "|      119|    3091|\n",
      "|      120|    6396|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT server_id, count(*)\n",
    "                    FROM utilization\n",
    "                    WHERE cpu_utilization >= 0.5\n",
    "                    GROUP BY server_id\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10457c",
   "metadata": {},
   "source": [
    "- order by descending count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e635e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|count(1)|\n",
      "+---------+--------+\n",
      "|      142|   10000|\n",
      "|      123|   10000|\n",
      "|      112|   10000|\n",
      "|      113|   10000|\n",
      "|      118|   10000|\n",
      "|      104|   10000|\n",
      "|      121|   10000|\n",
      "|      102|   10000|\n",
      "|      133|   10000|\n",
      "|      139|   10000|\n",
      "|      137|   10000|\n",
      "|      145|   10000|\n",
      "|      146|   10000|\n",
      "|      149|   10000|\n",
      "|      148|   10000|\n",
      "|      103|   10000|\n",
      "|      101|   10000|\n",
      "|      108|   10000|\n",
      "|      126|    9604|\n",
      "|      144|    9490|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT server_id, count(*)\n",
    "                    FROM utilization\n",
    "                    WHERE cpu_utilization >= 0.5\n",
    "                    GROUP BY server_id\n",
    "                    ORDER BY count(*) DESC\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a1a62",
   "metadata": {},
   "source": [
    "#### Basic stats about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c350d6",
   "metadata": {},
   "source": [
    "- find the min, max, and mean of session count of each server_id that had over 50% CPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7a31931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+----------------------------+\n",
      "|server_id|min(session_count)|max(session_count)|round(avg(session_count), 2)|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "|      142|                60|                95|                       77.81|\n",
      "|      123|                63|                98|                       80.84|\n",
      "|      112|                62|                97|                       79.03|\n",
      "|      113|                68|               103|                       85.94|\n",
      "|      118|                63|                98|                       80.92|\n",
      "|      104|                61|                96|                       78.75|\n",
      "|      121|                60|                95|                        77.8|\n",
      "|      102|                66|               101|                       83.22|\n",
      "|      133|                65|               100|                       82.99|\n",
      "|      139|                61|                96|                        78.8|\n",
      "|      137|                64|                99|                        81.9|\n",
      "|      145|                68|               103|                       85.75|\n",
      "|      146|                60|                95|                        77.8|\n",
      "|      149|                64|                99|                       81.96|\n",
      "|      148|                64|                99|                       81.27|\n",
      "|      103|                66|               101|                       83.58|\n",
      "|      101|                70|               105|                       87.33|\n",
      "|      108|                65|               100|                       82.26|\n",
      "|      126|                58|                93|                       75.25|\n",
      "|      144|                57|                92|                       74.85|\n",
      "+---------+------------------+------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"\"\"\n",
    "                    SELECT \n",
    "                        server_id, min(session_count), max(session_count), round(avg(session_count), 2)\n",
    "                    FROM \n",
    "                        utilization\n",
    "                    WHERE \n",
    "                        cpu_utilization >= 0.5\n",
    "                    GROUP BY \n",
    "                        server_id\n",
    "                    ORDER BY \n",
    "                        count(*) DESC\n",
    "                    \"\"\")\n",
    "\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e559e",
   "metadata": {},
   "source": [
    "### Joining table using SQL in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7efb62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util = spark.read.format(\"json\").load(json_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca075dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"utilization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2de439",
   "metadata": {},
   "source": [
    "- load the file countaining server names to join with the utilization table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a673f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server_path = path + '/server_name.csv'\n",
    "df_server = spark.read.format(\"csv\").load(df_server_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9a6055a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "|      110| 110 Server|\n",
      "|      111| 111 Server|\n",
      "|      112| 112 Server|\n",
      "|      113| 113 Server|\n",
      "|      114| 114 Server|\n",
      "|      115| 115 Server|\n",
      "|      116| 116 Server|\n",
      "|      117| 117 Server|\n",
      "|      118| 118 Server|\n",
      "|      119| 119 Server|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_server.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04bc7d",
   "metadata": {},
   "source": [
    "- create server name table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1970b285",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server.createOrReplaceTempView(\"servers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc939038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "|      110|\n",
      "|      111|\n",
      "|      112|\n",
      "|      113|\n",
      "|      114|\n",
      "|      115|\n",
      "|      116|\n",
      "|      117|\n",
      "|      118|\n",
      "|      119|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_count = spark.sql(\"\"\"\n",
    "                     SELECT DISTINCT server_id\n",
    "                     FROM servers\n",
    "                     ORDER BY server_id\n",
    "                     \"\"\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "235099d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(server_id)|max(server_id)|\n",
      "+--------------+--------------+\n",
      "|           100|           149|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT min(server_id), max(server_id)\n",
    "            FROM utilization\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36801aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "|      106| 106 Server|           32|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_join = spark.sql(\"\"\"\n",
    "                     SELECT u.server_id, s.server_name, u.session_count\n",
    "                     FROM utilization u\n",
    "                     INNER JOIN servers s\n",
    "                     ON u.server_id = s.server_id\n",
    "                     ORDER BY session_count\n",
    "                     \"\"\")\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3528c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
